{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def antonym_uppercase(\n",
    "    load_dir: str = 'data/base/antonym.json',    \n",
    "    save_path: str = 'data/composite/antonym-uppercase.json'    \n",
    "):\n",
    "\n",
    "    with open(load_dir, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    updated_data = []\n",
    "    for d in data:\n",
    "        entry = {\n",
    "            'input': d['input'],\n",
    "            'output': d['output'].capitalize(),\n",
    "            't1': d['output'],\n",
    "            't2': d['input'].capitalize(),\n",
    "        }\n",
    "        updated_data.append(entry)\n",
    "\n",
    "    with open(save_path, 'w') as outfile:\n",
    "        json.dump(updated_data, outfile, indent=4)\n",
    "\n",
    "antonym_uppercase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_uppercase(\n",
    "    load_dir: str = 'data/base/synonym.json',    \n",
    "    save_path: str = 'data/composite/synonym-uppercase.json'    \n",
    "):\n",
    "\n",
    "    with open(load_dir, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    updated_data = []\n",
    "    for d in data:\n",
    "        entry = {\n",
    "            'input': d['input'],\n",
    "            'output': d['output'].capitalize(),\n",
    "            't1': d['output'],\n",
    "            't2': d['input'].capitalize(),\n",
    "        }\n",
    "        updated_data.append(entry)\n",
    "\n",
    "    with open(save_path, 'w') as outfile:\n",
    "        json.dump(updated_data, outfile, indent=4)\n",
    "\n",
    "synonym_uppercase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def landmark_country_lowercase(\n",
    "    load_dir: str = 'data/base/landmark_country.json',    \n",
    "    save_path: str = 'data/composite/landmark_country-lowercase.json'    \n",
    "):\n",
    "\n",
    "    with open(load_dir, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    updated_data = []\n",
    "    for d in data:\n",
    "        entry = {\n",
    "            'input': d['input'],\n",
    "            'output': d['output'].lower(),\n",
    "            't1': d['output'],\n",
    "            't2': d['input'].lower(),\n",
    "        }\n",
    "        updated_data.append(entry)\n",
    "\n",
    "    with open(save_path, 'w') as outfile:\n",
    "        json.dump(updated_data, outfile, indent=4)\n",
    "\n",
    "landmark_country_lowercase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_capital_lowercase(\n",
    "    load_dir: str = 'data/base/country_capital.json',    \n",
    "    save_path: str = 'data/composite/country_capital-lowercase.json'    \n",
    "):\n",
    "\n",
    "    with open(load_dir, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    updated_data = []\n",
    "    for d in data:\n",
    "        entry = {\n",
    "            'input': d['input'],\n",
    "            'output': d['output'].lower(),\n",
    "            't1': d['output'],\n",
    "            't2': d['input'].lower(),\n",
    "        }\n",
    "        updated_data.append(entry)\n",
    "\n",
    "    with open(save_path, 'w') as outfile:\n",
    "        json.dump(updated_data, outfile, indent=4)\n",
    "\n",
    "country_capital_lowercase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_company_lowercase(\n",
    "    load_dir: str = 'data/base/product_company.json',    \n",
    "    save_path: str = 'data/composite/product_company-lowercase.json'    \n",
    "):\n",
    "\n",
    "    with open(load_dir, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    updated_data = []\n",
    "    for d in data:\n",
    "        entry = {\n",
    "            'input': d['input'],\n",
    "            'output': d['output'].lower(),\n",
    "            't1': d['output'],\n",
    "            't2': d['input'].lower(),\n",
    "        }\n",
    "        updated_data.append(entry)\n",
    "\n",
    "    with open(save_path, 'w') as outfile:\n",
    "        json.dump(updated_data, outfile, indent=4)\n",
    "\n",
    "product_company_lowercase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_last_landmark_country(\n",
    "    load_dir: str = 'data/base/landmark_country.json',    \n",
    "    save_path: str = 'data/composite/choose_last-landmark_country.json'    \n",
    "):\n",
    "\n",
    "    with open(load_dir, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    updated_data = []\n",
    "    for d in data:\n",
    "        random_pairs = np.random.choice(len(data), 2, replace=False)\n",
    "        entry = {\n",
    "            'input': f\"{data[random_pairs[0]]['input']}, {data[random_pairs[1]]['input']}, {d['input']}\",\n",
    "            'output': d['output'],\n",
    "            't1': d['input'],\n",
    "            't2': data[random_pairs[0]]['output'],\n",
    "        }\n",
    "        updated_data.append(entry)\n",
    "\n",
    "    with open(save_path, 'w') as outfile:\n",
    "        json.dump(updated_data, outfile, indent=4)\n",
    "\n",
    "\n",
    "choose_last_landmark_country()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_last_country_capital(\n",
    "    load_dir: str = 'data/base/country_capital.json',    \n",
    "    save_path: str = 'data/composite/choose_last-country_capital.json'    \n",
    "):\n",
    "\n",
    "    with open(load_dir, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    updated_data = []\n",
    "    for d in data:\n",
    "        random_pairs = np.random.choice(len(data), 2, replace=False)\n",
    "        entry = {\n",
    "            'input': f\"{data[random_pairs[0]]['input']}, {data[random_pairs[1]]['input']}, {d['input']}\",\n",
    "            'output': d['output'],\n",
    "            't1': d['input'],\n",
    "            't2': data[random_pairs[0]]['output'],\n",
    "        }\n",
    "        updated_data.append(entry)\n",
    "\n",
    "    with open(save_path, 'w') as outfile:\n",
    "        json.dump(updated_data, outfile, indent=4)\n",
    "\n",
    "\n",
    "choose_last_country_capital()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_last_uppercase(\n",
    "    load_dir: str = 'data/base/choose_last.json',    \n",
    "    save_path: str = 'data/composite/choose_last-uppercase.json'    \n",
    "):\n",
    "\n",
    "    with open(load_dir, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    updated_data = []\n",
    "    for d in data:\n",
    "        split_word = d['input'].split(', ')\n",
    "        entry = {\n",
    "            'input': d['input'],\n",
    "            'output': d['output'].capitalize(),\n",
    "            't1': d['output'],\n",
    "            't2': split_word[0].capitalize(),\n",
    "        }\n",
    "        updated_data.append(entry)\n",
    "\n",
    "    with open(save_path, 'w') as outfile:\n",
    "        json.dump(updated_data, outfile, indent=4)\n",
    "\n",
    "\n",
    "choose_last_uppercase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_last_first_letter(\n",
    "    load_dir: str = 'data/base/choose_last.json',    \n",
    "    save_path: str = 'data/composite/choose_last-first_letter.json'    \n",
    "):\n",
    "\n",
    "    with open(load_dir, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    updated_data = []\n",
    "    for d in data:\n",
    "        split_word = d['input'].split(', ')\n",
    "        entry = {\n",
    "            'input': d['input'],\n",
    "            'output': d['output'][0],\n",
    "            't1': d['output'],\n",
    "            't2': split_word[0][0],\n",
    "        }\n",
    "        updated_data.append(entry)\n",
    "\n",
    "    with open(save_path, 'w') as outfile:\n",
    "        json.dump(updated_data, outfile, indent=4)\n",
    "\n",
    "\n",
    "choose_last_first_letter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_data_dir = \"data/AntSynNET/dataset\"\n",
    "    splits = [\"train\", \"val\", \"test\"]\n",
    "    types = [\"adjective\", \"noun\", \"verb\"]\n",
    "\n",
    "    \n",
    "    for t in types:\n",
    "        word_list = []\n",
    "        for s in splits:\n",
    "            input_file = open(f'{input_data_dir}/{t}-pairs.{s}', \"r\")\n",
    "            for line in input_file:\n",
    "                word1, word2, num = line.split()\n",
    "                word_list.append(word1)\n",
    "                word_list.append(word2)\n",
    "            \n",
    "        with open(f'data/base/vocab/{t}.json', 'w') as outfile:\n",
    "            json.dump(word_list, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/base/vocab/adjective.json', \"r\") as f:\n",
    "    adj_orign = json.load(f)\n",
    "with open('data/base/vocab/noun.json', \"r\") as f:\n",
    "    n_orign = json.load(f)\n",
    "with open('data/base/vocab/verb.json', \"r\") as f:\n",
    "    v_orign = json.load(f)\n",
    "\n",
    "with open('data/base/antonym.json', \"r\") as f:\n",
    "    antonym_orign = json.load(f)\n",
    "\n",
    "adj_data = []\n",
    "v_data = []\n",
    "n_data = []\n",
    "for d in antonym_orign:\n",
    "    if (d['input'] in adj_orign) and (d['input'] not in n_orign) and (d['input'] not in v_orign) and (d['output'] in adj_orign) and (d['output'] not in n_orign) and (d['output'] not in v_orign):\n",
    "        adj_data.append(d)\n",
    "    if (d['input'] in v_orign) and (d['input'] not in n_orign) and (d['input'] not in adj_orign) and (d['output'] in v_orign) and (d['output'] not in n_orign) and (d['output'] not in adj_orign):\n",
    "        v_data.append(d)\n",
    "    if (d['input'] in n_orign) and (d['input'] not in v_orign) and (d['input'] not in adj_orign) and (d['output'] in n_orign) and (d['output'] not in v_orign) and (d['output'] not in adj_orign):\n",
    "        n_data.append(d)\n",
    "\n",
    "with open('./data/base/adj_antonym.json', 'w') as outfile:\n",
    "    json.dump(adj_data, outfile, indent=4)\n",
    "with open('./data/base/noun_antonym.json', 'w') as outfile:\n",
    "    json.dump(n_data, outfile, indent=4)\n",
    "with open('./data/base/verb_antonym.json', 'w') as outfile:\n",
    "    json.dump(v_data, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/base/vocab/adjective.json', \"r\") as f:\n",
    "    adj_orign = json.load(f)\n",
    "with open('data/base/vocab/noun.json', \"r\") as f:\n",
    "    n_orign = json.load(f)\n",
    "with open('data/base/vocab/verb.json', \"r\") as f:\n",
    "    v_orign = json.load(f)\n",
    "\n",
    "with open('data/base/synonym.json', \"r\") as f:\n",
    "    synonym_orign = json.load(f)\n",
    "\n",
    "adj_data = []\n",
    "v_data = []\n",
    "n_data = []\n",
    "for d in synonym_orign:\n",
    "    if (d['input'] in adj_orign) and (d['input'] not in n_orign) and (d['input'] not in v_orign) and (d['output'] in adj_orign) and (d['output'] not in n_orign) and (d['output'] not in v_orign):\n",
    "        adj_data.append(d)\n",
    "    if (d['input'] in v_orign) and (d['input'] not in n_orign) and (d['input'] not in adj_orign) and (d['output'] in v_orign) and (d['output'] not in n_orign) and (d['output'] not in adj_orign):\n",
    "        v_data.append(d)\n",
    "    if (d['input'] in n_orign) and (d['input'] not in v_orign) and (d['input'] not in adj_orign) and (d['output'] in n_orign) and (d['output'] not in v_orign) and (d['output'] not in adj_orign):\n",
    "        n_data.append(d)\n",
    "\n",
    "with open('./data/base/adj_synonym.json', 'w') as outfile:\n",
    "    json.dump(adj_data, outfile, indent=4)\n",
    "with open('./data/base/noun_synonym.json', 'w') as outfile:\n",
    "    json.dump(n_data, outfile, indent=4)\n",
    "with open('./data/base/verb_synonym.json', 'w') as outfile:\n",
    "    json.dump(v_data, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjective_v_verb_antonym(\n",
    "    save_path: str = 'data/composite/adjective_v_verb-antonym.json'    \n",
    "):\n",
    "\n",
    "    with open(\"data/base/adj_antonym.json\", \"r\") as f:\n",
    "        adj_data = json.load(f)\n",
    "    with open(\"data/base/verb_antonym.json\", \"r\") as f:\n",
    "        verb_data = json.load(f)\n",
    "    \n",
    "    updated_data = []\n",
    "    for d in adj_data:\n",
    "        chosen_two = random.sample(verb_data, 2)\n",
    "        combined = [d] + chosen_two\n",
    "        random.shuffle(combined)\n",
    "\n",
    "        entry = {\n",
    "            'input': f\"{combined[0]['input']}, {combined[1]['input']}, {combined[2]['input']}\",\n",
    "            'output': d['output'],\n",
    "            't1': d['input'],\n",
    "            't2': combined[0]['output'],\n",
    "        }\n",
    "        updated_data.append(entry)\n",
    "\n",
    "    with open(save_path, 'w') as outfile:\n",
    "        json.dump(updated_data, outfile, indent=4)\n",
    "\n",
    "\n",
    "adjective_v_verb_antonym()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjective_v_verb_synonym(\n",
    "    save_path: str = 'data/composite/adjective_v_verb-synonym.json'    \n",
    "):\n",
    "\n",
    "    with open(\"data/base/adj_synonym.json\", \"r\") as f:\n",
    "        adj_data = json.load(f)\n",
    "    with open(\"data/base/verb_synonym.json\", \"r\") as f:\n",
    "        verb_data = json.load(f)\n",
    "    \n",
    "    updated_data = []\n",
    "    for d in adj_data:\n",
    "        chosen_two = random.sample(verb_data, 2)\n",
    "        combined = [d] + chosen_two\n",
    "        random.shuffle(combined)\n",
    "\n",
    "        entry = {\n",
    "            'input': f\"{combined[0]['input']}, {combined[1]['input']}, {combined[2]['input']}\",\n",
    "            'output': d['output'],\n",
    "            't1': d['input'],\n",
    "            't2': combined[0]['output'],\n",
    "        }\n",
    "        updated_data.append(entry)\n",
    "\n",
    "    with open(save_path, 'w') as outfile:\n",
    "        json.dump(updated_data, outfile, indent=4)\n",
    "\n",
    "\n",
    "adjective_v_verb_synonym()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import time\n",
    "from inspect import iscoroutine\n",
    "from itertools import islice\n",
    "from pathlib import Path\n",
    "\n",
    "from googletrans import Translator        # your async fork\n",
    "# If your fork also exposes AsyncTranslator you can import that instead.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "BATCH_SIZE    = 100\n",
    "OUT_PATH      = Path(\"data/translations.json\")\n",
    "SLEEP_BETWEEN = 0.2      # seconds between requests\n",
    "\n",
    "# ---------- helpers -----------------------------------------\n",
    "def chunked(it, size):\n",
    "    it = iter(it)\n",
    "    while (batch := list(islice(it, size))):\n",
    "        yield batch\n",
    "\n",
    "async def translate_batch(translator, batch):\n",
    "    \"\"\"Returns (fr_list, es_list) for one batch, awaiting if needed.\"\"\"\n",
    "    fr_task = translator.translate(batch, dest=\"fr\")\n",
    "    es_task = translator.translate(batch, dest=\"es\")\n",
    "\n",
    "    # In the async fork both are coroutines; in rc1 they’re lists already.\n",
    "    if iscoroutine(fr_task) or iscoroutine(es_task):\n",
    "        fr_task, es_task = await asyncio.gather(fr_task, es_task)\n",
    "\n",
    "    return fr_task, es_task\n",
    "\n",
    "async def translate_word_list_async(words, batch_size=BATCH_SIZE):\n",
    "    translator = Translator()\n",
    "    result = {}\n",
    "\n",
    "    for batch in chunked(words, batch_size):\n",
    "        try:\n",
    "            fr_batch, es_batch = await translate_batch(translator, batch)\n",
    "        except Exception as exc:\n",
    "            print(f\"⚠️  error on batch starting with {batch[0]!r}: {exc}. Retrying …\")\n",
    "            time.sleep(2)\n",
    "            fr_batch, es_batch = await translate_batch(translator, batch)\n",
    "\n",
    "        for src, fr, es in zip(batch, fr_batch, es_batch):\n",
    "            result[src] = {\"fr\": fr.text, \"es\": es.text}\n",
    "\n",
    "        print(f\"✓ {len(batch)} done  (total {len(result)})\")\n",
    "        await asyncio.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "    # Many async forks need explicit shutdown:\n",
    "    if hasattr(translator, \"close\"):\n",
    "        await translator.close()\n",
    "\n",
    "    return result\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    words = []   # your pre‑loaded list\n",
    "    with open('data/base/antonym.json', \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    for d in data:\n",
    "        words.append(d['input'])\n",
    "        words.append(d['output'])\n",
    "        \n",
    "    with open('data/base/landmark_country.json', \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    for d in data:\n",
    "        words.append(d['input'])\n",
    "        words.append(d['output'])\n",
    "\n",
    "    print(f\"Total number: {len(words)}.\")\n",
    "\n",
    "    # ---------- run: script vs. notebook ---------------------\n",
    "    try:\n",
    "        # Works in a plain Python script\n",
    "        translations = asyncio.run(translate_word_list_async(words))\n",
    "    except RuntimeError:\n",
    "        # Already inside an event loop (e.g. Jupyter) → just await\n",
    "        import nest_asyncio, IPython\n",
    "        nest_asyncio.apply()                     # lets you re‑enter the loop\n",
    "        translations = await translate_word_list_async(words)  # type: ignore\n",
    "\n",
    "    OUT_PATH.write_text(\n",
    "        json.dumps(translations, ensure_ascii=False, indent=2),\n",
    "        encoding=\"utf‑8\",\n",
    "    )\n",
    "    print(f\"\\nAll done — {len(translations)} entries written to {OUT_PATH.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def antonym_english_french( \n",
    "    load_dir: str = 'data/base/antonym.json',    \n",
    "    save_path: str = 'data/composite/antonym-english_french.json'    \n",
    "):\n",
    "\n",
    "    with open(load_dir, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open(\"data/translations.json\", \"r\") as f:\n",
    "        translations = json.load(f)\n",
    "\n",
    "    updated_data = []\n",
    "    for d in data:\n",
    "        entry = {\n",
    "            'input': d['input'],\n",
    "            'output': translations[d['output']]['fr'],\n",
    "            't1': d['output'],\n",
    "            't2': translations[d['input']]['fr'],\n",
    "        }\n",
    "        updated_data.append(entry)\n",
    "\n",
    "    with open(save_path, 'w') as outfile:\n",
    "        json.dump(updated_data, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "antonym_english_french()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def antonym_english_spanish(\n",
    "    load_dir: str = 'data/base/antonym.json',    \n",
    "    save_path: str = 'data/composite/antonym-english_spanish.json'    \n",
    "):\n",
    "\n",
    "    with open(load_dir, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open(\"data/translations.json\", \"r\") as f:\n",
    "        translations = json.load(f)\n",
    "\n",
    "    updated_data = []\n",
    "    for d in data:\n",
    "        entry = {\n",
    "            'input': d['input'],\n",
    "            'output': translations[d['output']]['es'],\n",
    "            't1': d['output'],\n",
    "            't2': translations[d['input']]['es'],\n",
    "        }\n",
    "        updated_data.append(entry)\n",
    "\n",
    "    with open(save_path, 'w') as outfile:\n",
    "        json.dump(updated_data, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "antonym_english_spanish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def landmark_country_english_french(\n",
    "    load_dir: str = 'data/base/landmark_country.json',    \n",
    "    save_path: str = 'data/composite/landmark_country-english_french.json'    \n",
    "):\n",
    "\n",
    "    with open(load_dir, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open(\"data/translations.json\", \"r\") as f:\n",
    "        translations = json.load(f)\n",
    "\n",
    "    updated_data = []\n",
    "    for d in data:\n",
    "        entry = {\n",
    "            'input': d['input'],\n",
    "            'output': translations[d['output']]['fr'],\n",
    "            't1': d['output'],\n",
    "            't2': translations[d['input']]['fr'],\n",
    "        }\n",
    "        updated_data.append(entry)\n",
    "\n",
    "    with open(save_path, 'w') as outfile:\n",
    "        json.dump(updated_data, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "landmark_country_english_french()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def landmark_country_english_spanish(\n",
    "    load_dir: str = 'data/base/landmark_country.json',    \n",
    "    save_path: str = 'data/composite/landmark_country-english_spanish.json'    \n",
    "):\n",
    "\n",
    "    with open(load_dir, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open(\"data/translations.json\", \"r\") as f:\n",
    "        translations = json.load(f)\n",
    "\n",
    "    updated_data = []\n",
    "    for d in data:\n",
    "        entry = {\n",
    "            'input': d['input'],\n",
    "            'output': translations[d['output']]['es'],\n",
    "            't1': d['output'],\n",
    "            't2': translations[d['input']]['es'],\n",
    "        }\n",
    "        updated_data.append(entry)\n",
    "\n",
    "    with open(save_path, 'w') as outfile:\n",
    "        json.dump(updated_data, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "landmark_country_english_spanish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zpyMI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
